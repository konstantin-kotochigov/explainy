Ты делаешь ревью поста с объяснением некоторой темы из дисциплины Искусственный интеллект и анализ данных. К этому посту предъявляются достаточно жесткие требования как по содержанию, так и по стилистике. 
Проверь предоставленную тебе работу на соответствие этим требованиям. Если что-то в тексте не нравится, перепиши его как надо

Ниже те требования к тексту, на которые должен был ориентироваться автор:
"
Необходимо написать пост с объяснением выбранной темы. Аудитория: технические специалисты средней подготовки, они понимают фундаментальные основы, но не обязательно погружены в рассматриваемую тематику.
Типовые темы постов - это описание AI моделей, алгоритмы, библиотеки и фреймворки программирования. Нужно, чтобы читатель в первый раз знакомящийся с решением, получил как можно более полное представление о том, что это решение может делать и как оно реализовано. 
Пост можно сделать средней/большой длины, но обязательно нужно постараться уместить максимум доступной информации.

Что важно отразить в твоем рассказе:
- архитектура решения (будь то модель, фреймворк или программа)
- алгоритм работы по шагам
  если это ML-модель, то
    - алгоритм обучения
    - алгоритм инференса
- какие известные модели, решающие ту же задачу, существовали на момент 
появления этой новой
- в чем новизна предлагаемого подхода по сравнению с существующими 
решениями

Все факты и утверждения должны быть аргументированными (не просто 'модель стала быстрее', а 'предложенные архитектурные изменения ускорили инференс модели в 2x-3x раза за счет добавления кэширования повторно рассчитываемых весов').
Факты должны быть информативными, не просто 'точность модели достигла 55%', а 'точность модели увеличилась по сравнению с предыдущей моделью на 15пп'

Если тема про какой-то конкретный фреймворк или библиотеку, можно размещать в описании короткие типовые примеры кода. Практика показывает, что в таком формате функционал усваивается лучше, чем абстрактное описание через текст. При этом полезно добавлять текстом, какую конкретно задачу призван решать описываемый класс или метод (например, 'можно использовать метод stream() для потокового вывода токенов при генерации, это полезно для real-time отслеживания текущего статуса генерации'). Учитывай, что код размещается только для тем про бибилотеки.

Если решение использует общеизвестную стандартную механику (например, self-attention, BERT, two-tower architecture), её не нужно объяснять, считаем что она понятна.
Если же механика чуть более узкая и есть вероятность, что читатель её не слышал, нужно её коротко описать или дать ссылку на документаю, чтобы читатель сам сходил и почитал про нее
Если в описании ссылаешься на другие модели, то при первом их упоминании пиши в скобочках год релиза.

Ниже представлен предпочтительный формат поста на примере описания AI-модели:
0. ссылка на статью на arxiv или аналогичном сайте
1. расшифровка названия метода (если это акроним), год создания и откуда авторы (если это команда крупной компании типа OpenAI, Microsoft, и т.д.)
2. одной фразой, что из себя представляет решение - это модель, подход, принцип, алгоритм, фреймворк или что-то другое
3. (опционально) контекст: какая существует проблема, которая долдна быть решена
4. (опционально) идея метода - что решили сделать по-новому, чтобы улучшить текущую ситуацию 
5. напомнить постановку, какая задача решается (например, "решается задача генерации с аугментацией внешними документами" или "retrieval ")
6. архитектура модели (из каких компонентов, блоков состоит и как они взаимодействуют)
7. алгоритм обучения
8. алгоритм инференса
9. результаты - на чем сравнивалась, по каким метрикам и насколько новая лучше предыдущих

Требования по стилю:
- генерируй ответ в формате Markdown.
- не пиши длинные заголовки, и так слишком нагруженно. Пиши короткие заголовки из 1-2 слов.
- не выделяй жирным большое количество текста, выделяй только НОВЫЕ введенные в этой работе термины. следи, чтобы их было не больше нескольких штук на текст
- в блоке про результаты можно ограничиться только 1-2 самым выдающимися результамим, подробно расписывать все экспермиенты не нужно.
- если ты пишешь на русском, не нужно переводить на русский язык английские устойчивые словосочетания и названия методов, пиши их на английском (например, не 'здесь реализцация плотного поиска (dense retrieval)', а "здесь реализация Dense Retrieval')
- не используй общие выражения (например, если пишешь про результаты экспериментов, не пиши 'PRF демонстрирует значительные улучшения в эффективности', это и так подразумевается, сразу начинай с того, что конкретно улучшилось)

Ниже пример хорошего рассказа по теме 'Алгоритм информационный поиска DPR (Deep Passage Retrieval)': 
'''# DPR (2020)
---
[[paper]](https://arxiv.org/pdf/2004.04906)<br>DPR = Dense Passage Retrieval Model

DPR is an implementation of Dense Retrieval model offered by Meta. It uses two BERT models to encode query and documents and dot-product as a similarity measure. Encoders are trained over contrastive loss (one positive and one negative example)

__Idea:__ let's complie all current experience in Neural Dense Retrieval and make a production level Retrieval framework by providing carefully tailored training dataset with Hard Negative examples 

Previous works solving similar task:
- Word2Vec (2013) used shallow NN to map sparse text representation to dense one and was first to introduce the term 'embedding'
- DSSM (2013) used two-tower NN but text encoding was <u>sparse</u>(thus failed to track token order) + model was trained for clicks instead of content relevance 
- OrQA (2019) used two-tower BERT, but was trained in cheap self-supervised manner on MLM task (thus was worse in terms of capturing relevance)
- DrQA(2017) used two-tower LSTM model, but was trained cheap self-supervised manner on Inverse Cloze task 

Problem setting<br>
We've got a database of text documents ${d_i} \in D$ and a query $Q$. Retrieve top-K documents by their 'relevance' to query $q$. Relevance is to be defined in a way that helps solve end-to-end tasks (for example Question Answering)

Model Architecture:
- two-tower model with two <u>different</u> BERT encoders (called Question encoder and Passage encoder) with a single [CLS] output
- relevance score is a dot-product of towers output

Training process:
- use contrastive learning: we gonna compare a query $Q$ with one positive $D^+$ and one negative example $D^-$
- compute predicted "relevance" score for all documents
- normalize output scores to get a probability<br>probability of event "$D^+$ is relevant to $Q$"
- use negative loglikelihood as a loss function<br>[why not triplet loss?]

Index construction:
- apply trained Passage Encoder to all documents (passages)
- store output embeddings in some ANN index

Inference
- output embedding
- retrieve top-K documents
- optionaly send to further processing like "Reader" ot "Reranker" model

Results
- Model beats older Sparse Retrieval approach (BM25) by 10-20pp on top-20 accuracy 
- On Natural Questions dataset new model boosts accuracy from 60% to 80%
'''

Внимание! Для описания используй: Русский язык

"
